{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e22a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ce5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574770e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "mini_data_set = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7062aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_hdf(\"../data-frames/train_df.h5\")\n",
    "val_df = pd.read_hdf(\"../data-frames/val_df.h5\")\n",
    "test_df = pd.read_hdf(\"../data-frames/test_df.h5\")\n",
    "if mini_data_set:\n",
    "    train_df = train_df[:128]\n",
    "    val_df = val_df[:128]\n",
    "    test_df = test_df[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a7da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list(train_df.columns.values)\n",
    "ing_names = col_names[:-3]\n",
    "targets = ing_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d31b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([transforms.Resize((384,384)),\n",
    "                                       transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3ca65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(data.Dataset):\n",
    "    ''' Data wrapper for pytorch's data loader function '''\n",
    "    def __init__(self, image_df):\n",
    "        self.dataset = image_df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        c_row = self.dataset.iloc[index]\n",
    "        target_arr = []\n",
    "        for item in c_row[targets].values:\n",
    "            target_arr.append(item)\n",
    "        #print(target_arr)\n",
    "        image_path, target = c_row['path'], torch.from_numpy(np.array(target_arr)).float()  #image and target\n",
    "        #read as rgb image, resize and convert to range 0 to 1\n",
    "        image = cv2.imread(image_path, 1)\n",
    "        image = PIL.Image.fromarray(image)\n",
    "        image = image_transform(image) \n",
    "        return image, target, c_row['class_id']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ab9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.75, 1.0)),\n",
    "        transforms.RandomRotation(degrees=(0,45)),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.RandomHorizontalFlip(.5),\n",
    "        transforms.RandomPerspective(distortion_scale=0.25, p=.5),\n",
    "        transforms.RandomVerticalFlip(.5),\n",
    "        transforms.RandomGrayscale(.5), \n",
    "        transforms.RandomAdjustSharpness(p=0.5,sharpness_factor=0.5),\n",
    "        transforms.Resize((384,384)),\n",
    "        transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ae6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataLoader(data.Dataset):\n",
    "    ''' Data wrapper for pytorch's data loader function '''\n",
    "    def __init__(self, image_df):\n",
    "        self.dataset = image_df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        c_row = self.dataset.iloc[index]\n",
    "        target_arr = []\n",
    "        for item in c_row[targets].values:\n",
    "            target_arr.append(item)\n",
    "        #print(target_arr)\n",
    "        image_path, target = c_row['path'], torch.from_numpy(np.array(target_arr)).float()  #image and target\n",
    "        #read as rgb image, resize and convert to range 0 to 1\n",
    "        image = cv2.imread(image_path, 1)\n",
    "        image = PIL.Image.fromarray(image)\n",
    "        image = augmentation_transforms(image) \n",
    "        return image, target, c_row['class_id']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c87fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataLoader(train_df)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True, batch_size=batch_size, pin_memory=True)\n",
    "# create and then concatenate the augmented dataset\n",
    "augmented_train_dataset = AugmentedDataLoader(train_df)\n",
    "concat_train_dataset = data.ConcatDataset(datasets=[train_dataset, augmented_train_dataset])\n",
    "train_loader = torch.utils.data.DataLoader(concat_train_dataset,shuffle=True, batch_size=batch_size, \n",
    "                                           pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = CustomDataLoader(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,shuffle=True, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "test_dataset = CustomDataLoader(test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224d18fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x2143c9bcf70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4979ca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 384, 384])\n",
      "torch.Size([64, 227])\n",
      "torch.Size([64, 3, 384, 384])\n",
      "torch.Size([64, 227])\n",
      "128\n",
      "128\n",
      "128\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the shape of one batch\n",
    "train_images, train_labels, class_ids = next(iter(train_loader))\n",
    "test_images, test_labels, class_ids = next(iter(val_loader))\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd6b232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Resnet-50 model pretraned on ImageNet \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76882fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for name, child in model.named_children():\n",
    "    ct += 1\n",
    "    if ct < 8:\n",
    "        for name2, params in child.named_parameters():\n",
    "            params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcd43734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 False\n",
      "bn1 False\n",
      "relu True\n",
      "maxpool True\n",
      "layer1 False\n",
      "layer2 False\n",
      "layer3 False\n",
      "layer4 True\n",
      "avgpool True\n",
      "fc True\n"
     ]
    }
   ],
   "source": [
    "def check_freeze(model):\n",
    "    for name ,layer in model._modules.items():\n",
    "        s = []\n",
    "        for l in layer.parameters():\n",
    "          s.append(l.requires_grad)\n",
    "        print(name ,all(s))\n",
    "check_freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02834689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device=\"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "757ac587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=227, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move the model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7f5d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [64, 64, 192, 192]           9,408\n",
      "       BatchNorm2d-2         [64, 64, 192, 192]             128\n",
      "              ReLU-3         [64, 64, 192, 192]               0\n",
      "         MaxPool2d-4           [64, 64, 96, 96]               0\n",
      "            Conv2d-5           [64, 64, 96, 96]           4,096\n",
      "       BatchNorm2d-6           [64, 64, 96, 96]             128\n",
      "              ReLU-7           [64, 64, 96, 96]               0\n",
      "            Conv2d-8           [64, 64, 96, 96]          36,864\n",
      "       BatchNorm2d-9           [64, 64, 96, 96]             128\n",
      "             ReLU-10           [64, 64, 96, 96]               0\n",
      "           Conv2d-11          [64, 256, 96, 96]          16,384\n",
      "      BatchNorm2d-12          [64, 256, 96, 96]             512\n",
      "           Conv2d-13          [64, 256, 96, 96]          16,384\n",
      "      BatchNorm2d-14          [64, 256, 96, 96]             512\n",
      "             ReLU-15          [64, 256, 96, 96]               0\n",
      "       Bottleneck-16          [64, 256, 96, 96]               0\n",
      "           Conv2d-17           [64, 64, 96, 96]          16,384\n",
      "      BatchNorm2d-18           [64, 64, 96, 96]             128\n",
      "             ReLU-19           [64, 64, 96, 96]               0\n",
      "           Conv2d-20           [64, 64, 96, 96]          36,864\n",
      "      BatchNorm2d-21           [64, 64, 96, 96]             128\n",
      "             ReLU-22           [64, 64, 96, 96]               0\n",
      "           Conv2d-23          [64, 256, 96, 96]          16,384\n",
      "      BatchNorm2d-24          [64, 256, 96, 96]             512\n",
      "             ReLU-25          [64, 256, 96, 96]               0\n",
      "       Bottleneck-26          [64, 256, 96, 96]               0\n",
      "           Conv2d-27           [64, 64, 96, 96]          16,384\n",
      "      BatchNorm2d-28           [64, 64, 96, 96]             128\n",
      "             ReLU-29           [64, 64, 96, 96]               0\n",
      "           Conv2d-30           [64, 64, 96, 96]          36,864\n",
      "      BatchNorm2d-31           [64, 64, 96, 96]             128\n",
      "             ReLU-32           [64, 64, 96, 96]               0\n",
      "           Conv2d-33          [64, 256, 96, 96]          16,384\n",
      "      BatchNorm2d-34          [64, 256, 96, 96]             512\n",
      "             ReLU-35          [64, 256, 96, 96]               0\n",
      "       Bottleneck-36          [64, 256, 96, 96]               0\n",
      "           Conv2d-37          [64, 128, 96, 96]          32,768\n",
      "      BatchNorm2d-38          [64, 128, 96, 96]             256\n",
      "             ReLU-39          [64, 128, 96, 96]               0\n",
      "           Conv2d-40          [64, 128, 48, 48]         147,456\n",
      "      BatchNorm2d-41          [64, 128, 48, 48]             256\n",
      "             ReLU-42          [64, 128, 48, 48]               0\n",
      "           Conv2d-43          [64, 512, 48, 48]          65,536\n",
      "      BatchNorm2d-44          [64, 512, 48, 48]           1,024\n",
      "           Conv2d-45          [64, 512, 48, 48]         131,072\n",
      "      BatchNorm2d-46          [64, 512, 48, 48]           1,024\n",
      "             ReLU-47          [64, 512, 48, 48]               0\n",
      "       Bottleneck-48          [64, 512, 48, 48]               0\n",
      "           Conv2d-49          [64, 128, 48, 48]          65,536\n",
      "      BatchNorm2d-50          [64, 128, 48, 48]             256\n",
      "             ReLU-51          [64, 128, 48, 48]               0\n",
      "           Conv2d-52          [64, 128, 48, 48]         147,456\n",
      "      BatchNorm2d-53          [64, 128, 48, 48]             256\n",
      "             ReLU-54          [64, 128, 48, 48]               0\n",
      "           Conv2d-55          [64, 512, 48, 48]          65,536\n",
      "      BatchNorm2d-56          [64, 512, 48, 48]           1,024\n",
      "             ReLU-57          [64, 512, 48, 48]               0\n",
      "       Bottleneck-58          [64, 512, 48, 48]               0\n",
      "           Conv2d-59          [64, 128, 48, 48]          65,536\n",
      "      BatchNorm2d-60          [64, 128, 48, 48]             256\n",
      "             ReLU-61          [64, 128, 48, 48]               0\n",
      "           Conv2d-62          [64, 128, 48, 48]         147,456\n",
      "      BatchNorm2d-63          [64, 128, 48, 48]             256\n",
      "             ReLU-64          [64, 128, 48, 48]               0\n",
      "           Conv2d-65          [64, 512, 48, 48]          65,536\n",
      "      BatchNorm2d-66          [64, 512, 48, 48]           1,024\n",
      "             ReLU-67          [64, 512, 48, 48]               0\n",
      "       Bottleneck-68          [64, 512, 48, 48]               0\n",
      "           Conv2d-69          [64, 128, 48, 48]          65,536\n",
      "      BatchNorm2d-70          [64, 128, 48, 48]             256\n",
      "             ReLU-71          [64, 128, 48, 48]               0\n",
      "           Conv2d-72          [64, 128, 48, 48]         147,456\n",
      "      BatchNorm2d-73          [64, 128, 48, 48]             256\n",
      "             ReLU-74          [64, 128, 48, 48]               0\n",
      "           Conv2d-75          [64, 512, 48, 48]          65,536\n",
      "      BatchNorm2d-76          [64, 512, 48, 48]           1,024\n",
      "             ReLU-77          [64, 512, 48, 48]               0\n",
      "       Bottleneck-78          [64, 512, 48, 48]               0\n",
      "           Conv2d-79          [64, 256, 48, 48]         131,072\n",
      "      BatchNorm2d-80          [64, 256, 48, 48]             512\n",
      "             ReLU-81          [64, 256, 48, 48]               0\n",
      "           Conv2d-82          [64, 256, 24, 24]         589,824\n",
      "      BatchNorm2d-83          [64, 256, 24, 24]             512\n",
      "             ReLU-84          [64, 256, 24, 24]               0\n",
      "           Conv2d-85         [64, 1024, 24, 24]         262,144\n",
      "      BatchNorm2d-86         [64, 1024, 24, 24]           2,048\n",
      "           Conv2d-87         [64, 1024, 24, 24]         524,288\n",
      "      BatchNorm2d-88         [64, 1024, 24, 24]           2,048\n",
      "             ReLU-89         [64, 1024, 24, 24]               0\n",
      "       Bottleneck-90         [64, 1024, 24, 24]               0\n",
      "           Conv2d-91          [64, 256, 24, 24]         262,144\n",
      "      BatchNorm2d-92          [64, 256, 24, 24]             512\n",
      "             ReLU-93          [64, 256, 24, 24]               0\n",
      "           Conv2d-94          [64, 256, 24, 24]         589,824\n",
      "      BatchNorm2d-95          [64, 256, 24, 24]             512\n",
      "             ReLU-96          [64, 256, 24, 24]               0\n",
      "           Conv2d-97         [64, 1024, 24, 24]         262,144\n",
      "      BatchNorm2d-98         [64, 1024, 24, 24]           2,048\n",
      "             ReLU-99         [64, 1024, 24, 24]               0\n",
      "      Bottleneck-100         [64, 1024, 24, 24]               0\n",
      "          Conv2d-101          [64, 256, 24, 24]         262,144\n",
      "     BatchNorm2d-102          [64, 256, 24, 24]             512\n",
      "            ReLU-103          [64, 256, 24, 24]               0\n",
      "          Conv2d-104          [64, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-105          [64, 256, 24, 24]             512\n",
      "            ReLU-106          [64, 256, 24, 24]               0\n",
      "          Conv2d-107         [64, 1024, 24, 24]         262,144\n",
      "     BatchNorm2d-108         [64, 1024, 24, 24]           2,048\n",
      "            ReLU-109         [64, 1024, 24, 24]               0\n",
      "      Bottleneck-110         [64, 1024, 24, 24]               0\n",
      "          Conv2d-111          [64, 256, 24, 24]         262,144\n",
      "     BatchNorm2d-112          [64, 256, 24, 24]             512\n",
      "            ReLU-113          [64, 256, 24, 24]               0\n",
      "          Conv2d-114          [64, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-115          [64, 256, 24, 24]             512\n",
      "            ReLU-116          [64, 256, 24, 24]               0\n",
      "          Conv2d-117         [64, 1024, 24, 24]         262,144\n",
      "     BatchNorm2d-118         [64, 1024, 24, 24]           2,048\n",
      "            ReLU-119         [64, 1024, 24, 24]               0\n",
      "      Bottleneck-120         [64, 1024, 24, 24]               0\n",
      "          Conv2d-121          [64, 256, 24, 24]         262,144\n",
      "     BatchNorm2d-122          [64, 256, 24, 24]             512\n",
      "            ReLU-123          [64, 256, 24, 24]               0\n",
      "          Conv2d-124          [64, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-125          [64, 256, 24, 24]             512\n",
      "            ReLU-126          [64, 256, 24, 24]               0\n",
      "          Conv2d-127         [64, 1024, 24, 24]         262,144\n",
      "     BatchNorm2d-128         [64, 1024, 24, 24]           2,048\n",
      "            ReLU-129         [64, 1024, 24, 24]               0\n",
      "      Bottleneck-130         [64, 1024, 24, 24]               0\n",
      "          Conv2d-131          [64, 256, 24, 24]         262,144\n",
      "     BatchNorm2d-132          [64, 256, 24, 24]             512\n",
      "            ReLU-133          [64, 256, 24, 24]               0\n",
      "          Conv2d-134          [64, 256, 24, 24]         589,824\n",
      "     BatchNorm2d-135          [64, 256, 24, 24]             512\n",
      "            ReLU-136          [64, 256, 24, 24]               0\n",
      "          Conv2d-137         [64, 1024, 24, 24]         262,144\n",
      "     BatchNorm2d-138         [64, 1024, 24, 24]           2,048\n",
      "            ReLU-139         [64, 1024, 24, 24]               0\n",
      "      Bottleneck-140         [64, 1024, 24, 24]               0\n",
      "          Conv2d-141          [64, 512, 24, 24]         524,288\n",
      "     BatchNorm2d-142          [64, 512, 24, 24]           1,024\n",
      "            ReLU-143          [64, 512, 24, 24]               0\n",
      "          Conv2d-144          [64, 512, 12, 12]       2,359,296\n",
      "     BatchNorm2d-145          [64, 512, 12, 12]           1,024\n",
      "            ReLU-146          [64, 512, 12, 12]               0\n",
      "          Conv2d-147         [64, 2048, 12, 12]       1,048,576\n",
      "     BatchNorm2d-148         [64, 2048, 12, 12]           4,096\n",
      "          Conv2d-149         [64, 2048, 12, 12]       2,097,152\n",
      "     BatchNorm2d-150         [64, 2048, 12, 12]           4,096\n",
      "            ReLU-151         [64, 2048, 12, 12]               0\n",
      "      Bottleneck-152         [64, 2048, 12, 12]               0\n",
      "          Conv2d-153          [64, 512, 12, 12]       1,048,576\n",
      "     BatchNorm2d-154          [64, 512, 12, 12]           1,024\n",
      "            ReLU-155          [64, 512, 12, 12]               0\n",
      "          Conv2d-156          [64, 512, 12, 12]       2,359,296\n",
      "     BatchNorm2d-157          [64, 512, 12, 12]           1,024\n",
      "            ReLU-158          [64, 512, 12, 12]               0\n",
      "          Conv2d-159         [64, 2048, 12, 12]       1,048,576\n",
      "     BatchNorm2d-160         [64, 2048, 12, 12]           4,096\n",
      "            ReLU-161         [64, 2048, 12, 12]               0\n",
      "      Bottleneck-162         [64, 2048, 12, 12]               0\n",
      "          Conv2d-163          [64, 512, 12, 12]       1,048,576\n",
      "     BatchNorm2d-164          [64, 512, 12, 12]           1,024\n",
      "            ReLU-165          [64, 512, 12, 12]               0\n",
      "          Conv2d-166          [64, 512, 12, 12]       2,359,296\n",
      "     BatchNorm2d-167          [64, 512, 12, 12]           1,024\n",
      "            ReLU-168          [64, 512, 12, 12]               0\n",
      "          Conv2d-169         [64, 2048, 12, 12]       1,048,576\n",
      "     BatchNorm2d-170         [64, 2048, 12, 12]           4,096\n",
      "            ReLU-171         [64, 2048, 12, 12]               0\n",
      "      Bottleneck-172         [64, 2048, 12, 12]               0\n",
      "AdaptiveAvgPool2d-173           [64, 2048, 1, 1]               0\n",
      "          Linear-174                  [64, 227]         465,123\n",
      "================================================================\n",
      "Total params: 23,973,155\n",
      "Trainable params: 15,429,859\n",
      "Non-trainable params: 8,543,296\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 108.00\n",
      "Forward/backward pass size (MB): 11642.89\n",
      "Params size (MB): 91.45\n",
      "Estimated Total Size (MB): 11842.34\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksbha\\.conda\\envs\\cs7643-a4\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model.cuda()\n",
    "summary(model, (3, 384, 384), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15eb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent with momentum)\n",
    "# optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer = optim.Adam(model.fc.parameters(), lr=0.001, betas=[0.9, 0.999])  \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# specify learning rate scheduler (if there is no further decrease in loss for next 5 epochs \n",
    "# then lower the learning rate by 0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=0.005 )\n",
    "#scheduler=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f7d8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    https://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a11afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "def calculate_scores(all_outputs, all_targets, scores_dict):\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_targets = np.array(all_targets)\n",
    "    f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
    "    f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\n",
    "    f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\n",
    "    recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
    "    prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
    "    hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\n",
    "    \n",
    "    scores_dict[\"f1score_samples\"].append(f1score_samples)\n",
    "    scores_dict[\"f1score_macro\"].append(f1score_macro)\n",
    "    scores_dict[\"f1score_weighted\"].append(f1score_weighted)\n",
    "    scores_dict[\"recall\"].append(recall)\n",
    "    scores_dict[\"prec\"].append(prec)\n",
    "    scores_dict[\"hamming\"].append(hamming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3265fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, scores_dict, scheduler=None):\n",
    "    model.train()\n",
    "    # Record total loss\n",
    "    total_loss = 0.\n",
    "    total_accuracy = 0.\n",
    "    total = 0.\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    for inputs, labels,class_ids in tqdm(dataloader, desc='Training'): \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # predict targets\n",
    "        preds = torch.sigmoid(output).data >= 0.5\n",
    "        target_data = (labels==1.0)\n",
    "        \n",
    "      \n",
    "        total_loss += (loss.item())\n",
    "        accuracy = torch.sum((preds == target_data.to(device)).to(torch.float)).item()\n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "        total_batch = (labels.size(0) * labels.size(1))\n",
    "        total += total_batch\n",
    "        \n",
    "        # collect predictions and targets\n",
    "        for arr1,arr2 in zip(preds, target_data):\n",
    "            all_outputs.append(list(arr1.cpu().numpy()))\n",
    "            all_targets.append(list(arr2.cpu().numpy()))\n",
    "\n",
    "    calculate_scores(all_outputs, all_targets,scores_dict)\n",
    "    return  total_accuracy / float(total), total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5b66a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, scores_dict):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_accuracy = 0.\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    all_class_ids = []\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, class_ids in tqdm(dataloader, desc='Validation'):\n",
    "            inputs, labels, = inputs.to(device), labels.to(device)\n",
    "            all_class_ids.extend(class_ids.cpu().numpy())\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "                 \n",
    "            # predict targets\n",
    "            preds = torch.sigmoid(output).data >= 0.5\n",
    "            preds = preds.to(torch.float32)\n",
    "            target_data = (labels==1.0)\n",
    "           \n",
    "            total_loss += (loss.item())\n",
    "            accuracy = torch.sum((preds == target_data.to(device)).to(torch.float)).item()\n",
    "            total_accuracy += accuracy\n",
    "            \n",
    "            total_batch = (labels.size(0) * labels.size(1))\n",
    "            total += total_batch\n",
    "            \n",
    "            #collect predictions and targets\n",
    "            for arr1,arr2 in zip(preds, target_data):\n",
    "                all_outputs.append(list(arr1.cpu().numpy()))\n",
    "                all_targets.append(list(arr2.cpu().numpy()))\n",
    "                \n",
    "    calculate_scores(all_outputs, all_targets, scores_dict)              \n",
    "    return  total_accuracy / float(total), total_loss / len(dataloader), all_outputs, all_targets, all_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d84ba686",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point_path = r'../checkpoints/food-101.pt'\n",
    "train_scores_path = r'../checkpoints/train_scores.json'\n",
    "val_scores_path = r'../checkpoints/val_scores.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dca6fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_scores_dict(scores_dict):\n",
    "    scores_dict[\"f1score_samples\"]=[]\n",
    "    scores_dict[\"f1score_macro\"]=[]\n",
    "    scores_dict[\"f1score_weighted\"]=[]\n",
    "    scores_dict[\"recall\"]=[]\n",
    "    scores_dict[\"prec\"]=[]\n",
    "    scores_dict[\"hamming\"]=[]\n",
    "    scores_dict[\"avg_accuracy\"]=[]\n",
    "    scores_dict[\"avg_losses\"]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da63a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74a3c11463e43bdabedd2e666d663ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8831. Training Loss: 0.3062. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0a822ad37649a1ba5300801608e278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9799. Validation Loss: 0.1171. \n",
      "Validation loss decreased,  Saving model\n",
      "-----------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732489b13f6e4188b86c91f743d04728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000. Training Loss: 0.0105. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab97143ef4c478694554445c308d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9799. Validation Loss: 0.1538. \n",
      "-----------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fdeb5e442847788c6bc435a9192039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000. Training Loss: 0.0007. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7117f793f6413da402d39f9ae82a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9799. Validation Loss: 0.2171. \n",
      "-----------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b7aa6905ae4ed7bfb70a1cb4c3c111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000. Training Loss: 0.0001. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b603c1a39b44ee0a79cbab3936e79b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9799. Validation Loss: 0.2378. \n",
      "-----------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1f2caecaab4fe2b0cd24b8176ead42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000. Training Loss: 0.0000. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bde0b18ef5471d9e5f5f2800d486bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9799. Validation Loss: 0.2396. \n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "EPOCHS=5\n",
    "valid_loss_min = np.Inf\n",
    "train_scores_dict = dict()\n",
    "initialize_scores_dict(train_scores_dict)\n",
    "val_scores_dict = dict()\n",
    "initialize_scores_dict(val_scores_dict)\n",
    "for epoch_idx in range(EPOCHS):\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Epoch %d\" % (epoch_idx+1))\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    avg_train_accuracy, avg_train_loss = train(model, train_loader, optimizer, criterion, train_scores_dict)\n",
    "    print(\"Training Accuracy: %.4f. Training Loss: %.4f. \" % (avg_train_accuracy, avg_train_loss))\n",
    "\n",
    "    avg_val_accuracy, avg_val_loss, all_outputs, all_targets, all_class_ids = evaluate(model, val_loader, criterion, val_scores_dict)\n",
    "    print(\"Validation Accuracy: %.4f. Validation Loss: %.4f. \" % (avg_val_accuracy, avg_val_loss))\n",
    "    \n",
    "       \n",
    "    if avg_val_loss <= valid_loss_min:\n",
    "            checkpoint = {\"model\": model,\n",
    "                      \"criterion\": criterion,\n",
    "                      \"epochs\": epoch_idx,\n",
    "                      \"optimizer_state\": optimizer.state_dict(),\n",
    "                      \"model_state\": model.state_dict(),\n",
    "                      \"valid_loss_min\": avg_val_loss}\n",
    "            print(\"Validation loss decreased,  Saving model\")\n",
    "            torch.save(checkpoint, check_point_path)\n",
    "            valid_loss_min=avg_val_loss\n",
    "    \n",
    "    train_scores_dict[\"avg_accuracy\"].append(avg_train_accuracy)\n",
    "    train_scores_dict[\"avg_losses\"].append(avg_train_loss)\n",
    "    val_scores_dict[\"avg_accuracy\"].append(avg_val_accuracy)\n",
    "    val_scores_dict[\"avg_losses\"].append(avg_val_loss)\n",
    "    \n",
    "# save scores\n",
    "with open(train_scores_path, 'w') as t_handle:\n",
    "    json.dump(train_scores_dict, t_handle)\n",
    "with open(val_scores_path, 'w') as v_handle:\n",
    "    json.dump(val_scores_dict, v_handle)\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3cad0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1score_samples': [0.760278380296568, 1.0, 1.0, 1.0, 1.0], 'f1score_macro': [0.02858276717597583, 0.030837004405286344, 0.030837004405286344, 0.030837004405286344, 0.030837004405286344], 'f1score_weighted': [0.9268983069923591, 1.0, 1.0, 1.0, 1.0], 'recall': [0.8677455357142857, 1.0, 1.0, 1.0, 1.0], 'prec': [0.7496322765175103, 1.0, 1.0, 1.0, 1.0], 'hamming': [0.7493910407272044, 1.0, 1.0, 1.0, 1.0], 'avg_accuracy': [0.8830878303964758, 1.0, 1.0, 1.0, 1.0], 'avg_losses': [0.30619468726217747, 0.010536933259572834, 0.0007287617190741003, 0.00012676171900238842, 4.0379452457273146e-05]}\n",
      "{'f1score_samples': [0.6963541666666667, 0.6963541666666666, 0.6963541666666667, 0.6963541666666668, 0.6963541666666667], 'f1score_macro': [0.0250862611488466, 0.0250862611488466, 0.0250862611488466, 0.0250862611488466, 0.0250862611488466], 'f1score_weighted': [0.5617424616007516, 0.5617424616007516, 0.5617424616007516, 0.5617424616007516, 0.5617424616007516], 'recall': [0.689453125, 0.689453125, 0.689453125, 0.689453125, 0.689453125], 'prec': [0.7042410714285714, 0.7042410714285714, 0.7042410714285715, 0.7042410714285715, 0.7042410714285714], 'hamming': [0.649639423076923, 0.6496394230769231, 0.6496394230769231, 0.649639423076923, 0.6496394230769231], 'avg_accuracy': [0.9799352973568282, 0.9799352973568282, 0.9799352973568282, 0.9799352973568282, 0.9799352973568282], 'avg_losses': [0.11712759360671043, 0.15377507358789444, 0.21714457869529724, 0.23775140196084976, 0.2395680844783783]}\n"
     ]
    }
   ],
   "source": [
    "with open(train_scores_path, 'r') as t_handle:\n",
    "    train_scores_dict = json.load(t_handle)\n",
    "with open(val_scores_path, 'r') as v_handle:\n",
    "    val_scores_dict = json.load(v_handle)\n",
    "print(train_scores_dict)\n",
    "print(val_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b9612e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_freeze(model):\n",
    "    for name ,layer in model._modules.items():\n",
    "        s = []\n",
    "        for l in layer.parameters():\n",
    "          s.append(l.requires_grad)\n",
    "        print(name ,all(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75fc16f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=227, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "test_model = models.resnet50(pretrained=True)\n",
    "num_ftrs = test_model.fc.in_features\n",
    "test_model.fc = nn.Linear(num_ftrs, len(targets))\n",
    "\n",
    "save_path = r'./checkpoints/food-101.pt'\n",
    "checkpoint = torch.load(save_path)\n",
    "test_model.load_state_dict(checkpoint['model_state'])\n",
    "test_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5a0c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 True\n",
      "bn1 True\n",
      "relu True\n",
      "maxpool True\n",
      "layer1 True\n",
      "layer2 True\n",
      "layer3 True\n",
      "layer4 True\n",
      "avgpool True\n",
      "fc True\n"
     ]
    }
   ],
   "source": [
    "check_freeze(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2fbd021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfb1b8a9df341fbad3a0d010cf199be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scores_dict = dict()\n",
    "initialize_scores_dict(test_scores_dict)\n",
    "avg_test_accuracy, avg_test_loss, all_outputs, all_targets, all_class_ids = evaluate(test_model, test_loader, criterion, test_scores_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7982e82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1score_samples': [0.8038613519177972],\n",
       " 'f1score_macro': [0.8223669901338875],\n",
       " 'f1score_weighted': [0.8391371353984649],\n",
       " 'recall': [0.7918676595106183],\n",
       " 'prec': [0.8541864510103001],\n",
       " 'hamming': [0.7603885281627365],\n",
       " 'avg_accuracy': [],\n",
       " 'avg_losses': []}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d164d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{39: 0.5517733630081957,\n",
       " 77: 0.5550570611449737,\n",
       " 0: 0.6279234196927453,\n",
       " 82: 0.6413243318772154,\n",
       " 93: 0.7149085913760341,\n",
       " 59: 0.7218627088369649,\n",
       " 56: 0.7338429991613579,\n",
       " 8: 0.7367424174774294,\n",
       " 67: 0.7412380756375411,\n",
       " 37: 0.7516114829583519,\n",
       " 50: 0.7741754059449272,\n",
       " 99: 0.7744369683629663,\n",
       " 5: 0.7819973399252615,\n",
       " 46: 0.789743097015692,\n",
       " 49: 0.7979732462321973,\n",
       " 57: 0.8001899389317134,\n",
       " 87: 0.8005197376624624,\n",
       " 84: 0.8040777187481831,\n",
       " 9: 0.8091415750117049,\n",
       " 47: 0.8125684249509275,\n",
       " 10: 0.8138622093697752,\n",
       " 15: 0.8146330808829365,\n",
       " 53: 0.8194405745685454,\n",
       " 18: 0.8237230640257336,\n",
       " 36: 0.8276561438263274,\n",
       " 58: 0.828034723383405,\n",
       " 22: 0.8300966870348713,\n",
       " 66: 0.8361542579146333,\n",
       " 62: 0.8369515795436642,\n",
       " 73: 0.8373640908663847,\n",
       " 26: 0.8484663502828405,\n",
       " 85: 0.8488961143950757,\n",
       " 16: 0.8490842374640399,\n",
       " 80: 0.8527618704674506,\n",
       " 42: 0.8534819788001898,\n",
       " 98: 0.8591539252546864,\n",
       " 19: 0.8659187710798463,\n",
       " 4: 0.8683378261620622,\n",
       " 89: 0.8690079239784705,\n",
       " 11: 0.8783456476446986,\n",
       " 96: 0.8787794935545186,\n",
       " 52: 0.8808722375005709,\n",
       " 38: 0.8811855373586746,\n",
       " 74: 0.8825019517497412,\n",
       " 21: 0.8839021556268198,\n",
       " 94: 0.8843311375332074,\n",
       " 1: 0.8847703981209324,\n",
       " 48: 0.8878908332317518,\n",
       " 28: 0.8931417987679146,\n",
       " 41: 0.8966629904968747,\n",
       " 14: 0.8988888267644898,\n",
       " 55: 0.9032137375169008,\n",
       " 20: 0.9050226717861997,\n",
       " 61: 0.9071822281845013,\n",
       " 71: 0.9078830839169826,\n",
       " 13: 0.9099868218397723,\n",
       " 12: 0.9104484894385668,\n",
       " 44: 0.9110311038321443,\n",
       " 78: 0.9142765744989271,\n",
       " 81: 0.9162627425179464,\n",
       " 43: 0.9204923061573782,\n",
       " 92: 0.9211230350763598,\n",
       " 79: 0.9220001527673248,\n",
       " 60: 0.9220072680001926,\n",
       " 17: 0.9231562242503226,\n",
       " 35: 0.9260359281570875,\n",
       " 95: 0.9308958575636014,\n",
       " 32: 0.9316103106057486,\n",
       " 25: 0.9328006974409161,\n",
       " 3: 0.9334843852382216,\n",
       " 72: 0.936131882817097,\n",
       " 83: 0.9363060383620883,\n",
       " 34: 0.9380009953798123,\n",
       " 40: 0.9404484197173315,\n",
       " 7: 0.9428011014271149,\n",
       " 97: 0.9452888403803276,\n",
       " 2: 0.9459877720630494,\n",
       " 31: 0.9462213363485359,\n",
       " 65: 0.9466267738893966,\n",
       " 24: 0.948074151866273,\n",
       " 51: 0.9489113969507502,\n",
       " 76: 0.950012760328686,\n",
       " 86: 0.9508040770194855,\n",
       " 23: 0.9516229913320112,\n",
       " 6: 0.953874351476436,\n",
       " 70: 0.954051224168974,\n",
       " 27: 0.9554438932456719,\n",
       " 88: 0.9556127217616899,\n",
       " 100: 0.9561662300382469,\n",
       " 64: 0.9597915645009635,\n",
       " 45: 0.9602107601215246,\n",
       " 90: 0.9617605088549974,\n",
       " 29: 0.9628421906861359,\n",
       " 68: 0.9630828132577804,\n",
       " 91: 0.9645876257010775,\n",
       " 69: 0.9658404702477754,\n",
       " 75: 0.9704514794322058,\n",
       " 30: 0.9726285750921719,\n",
       " 54: 0.9791541382303419,\n",
       " 63: 0.9821509368940862,\n",
       " 33: 0.9929496332835724}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_class_ids))\n",
    "len(all_targets)\n",
    "unique_classes = set(all_class_ids)\n",
    "all_targets = np.array(all_targets)\n",
    "all_outputs = np.array(all_outputs)\n",
    "all_class_ids = np.array(all_class_ids)\n",
    "food_type_dict=dict()\n",
    "for entry in unique_classes:\n",
    "    class_indices = [i for i, e in enumerate(all_class_ids) if e == entry]\n",
    "    class_targets = all_targets[class_indices]\n",
    "    class_outputs = all_outputs[class_indices]\n",
    "    food_type_dict[entry]=f1_score(y_true=class_targets, y_pred=class_outputs, average='weighted')\n",
    "\n",
    "food_type_dict = {k: v for k, v in sorted(food_type_dict.items(), key=lambda item: item[1])}\n",
    "food_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89511554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "cs7643-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
